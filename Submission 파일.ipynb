{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fVCIlJ5fyfIe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy import stats\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# Constants\n",
        "ROOT_DIR = \"data\"\n",
        "RANDOM_STATE = 110\n",
        "\n",
        "# Load training data\n",
        "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
        "\n",
        "# Drop columns with only one unique value\n",
        "train_data = train_data.loc[:, train_data.nunique() > 1]\n",
        "\n",
        "# Replace 'OK' with NaN\n",
        "train_data.replace(\"OK\", np.nan, inplace=True)\n",
        "\n",
        "# Drop duplicated columns\n",
        "train_data = train_data.loc[:, ~train_data.T.duplicated()]\n",
        "\n",
        "# Drop unneeded columns\n",
        "drop_cols = [\n",
        "    'Equipment_Fill1', 'Equipment_Fill2', 'Chamber Temp. Judge Value_AutoClave',\n",
        "    'Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam'\n",
        "]\n",
        "train_data.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
        "\n",
        "# Convert specific columns to float\n",
        "float_columns = [\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
        "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'\n",
        "]\n",
        "train_data[float_columns] = train_data[float_columns].astype(float)\n",
        "\n",
        "# Feature/label split\n",
        "X = train_data.drop('target', axis=1)\n",
        "y = train_data['target']\n",
        "\n",
        "# Visualize missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(train_data.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"missing_values_heatmap.png\")\n",
        "plt.close()\n",
        "\n",
        "# Z-score normalization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Outlier removal using Z-score\n",
        "z_scores = stats.zscore(X_scaled)\n",
        "filtered_indices = (np.abs(z_scores) < 3).all(axis=1)\n",
        "X_filtered = X_scaled[filtered_indices]\n",
        "y_filtered = y[filtered_indices]\n",
        "\n",
        "# SMOTE for imbalance handling\n",
        "smote = SMOTE(random_state=RANDOM_STATE)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_filtered, y_filtered)\n",
        "\n",
        "# Remove highly correlated features\n",
        "X_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "correlation_matrix = X_df.corr()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"correlation_heatmap.png\")\n",
        "plt.close()\n",
        "\n",
        "high_corr_pairs = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
        "            high_corr_pairs.add(correlation_matrix.columns[i])\n",
        "X_df.drop(columns=high_corr_pairs, inplace=True)\n",
        "\n",
        "# Combine features and target\n",
        "train_data_final = pd.concat([X_df, pd.DataFrame(y_resampled, columns=['target'])], axis=1)\n",
        "\n",
        "# Try converting columns to int where possible\n",
        "features = []\n",
        "for col in train_data_final.columns:\n",
        "    try:\n",
        "        train_data_final[col] = train_data_final[col].astype(int)\n",
        "        features.append(col)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Final training set\n",
        "train_x = train_data_final[features]\n",
        "train_y = train_data_final['target']\n",
        "\n",
        "# GridSearchCV for CatBoost\n",
        "param_grid = {\n",
        "    'depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'iterations': [100, 200]\n",
        "}\n",
        "cat_model = CatBoostClassifier(random_state=RANDOM_STATE, verbose=0)\n",
        "grid_search = GridSearchCV(cat_model, param_grid, cv=3, scoring='f1_macro')\n",
        "grid_search.fit(train_x, train_y)\n",
        "\n",
        "# Best model\n",
        "model = grid_search.best_estimator_\n",
        "\n",
        "# Feature Importance Visualization\n",
        "feature_importances = model.get_feature_importance(Pool(train_x, label=train_y))\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=feature_importances, y=train_x.columns)\n",
        "plt.title(\"CatBoost Feature Importances (Optimized)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"feature_importance_optimized.png\")\n",
        "plt.close()\n",
        "\n",
        "# Predict on test set\n",
        "# Ensure df_test_x is preloaded\n",
        "for col in df_test_x.columns:\n",
        "    try:\n",
        "        df_test_x[col] = df_test_x[col].astype(int)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "test_pred = model.predict(df_test_x)\n",
        "\n",
        "# Prepare submission\n",
        "submission = pd.read_csv(\"submission.csv\")\n",
        "submission[\"target\"] = test_pred\n",
        "submission.to_csv(\"submission-optimized.csv\", index=False)"
      ]
    }
  ]
}